{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72955741-140c-4e7d-89fb-5627efeaa579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:37.433243Z",
     "iopub.status.busy": "2023-05-02T22:08:37.433131Z",
     "iopub.status.idle": "2023-05-02T22:08:39.461419Z",
     "shell.execute_reply": "2023-05-02T22:08:39.461041Z",
     "shell.execute_reply.started": "2023-05-02T22:08:37.433229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/02 19:08:38 WARN Utils: Your hostname, rig resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface enp6s0)\n",
      "23/05/02 19:08:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/02 19:08:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56e1d05-f6cb-45c1-9dc7-a92054bc8f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:39.462092Z",
     "iopub.status.busy": "2023-05-02T22:08:39.461896Z",
     "iopub.status.idle": "2023-05-02T22:08:40.646637Z",
     "shell.execute_reply": "2023-05-02T22:08:40.646350Z",
     "shell.execute_reply.started": "2023-05-02T22:08:39.462081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "voter_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('DATE', DateType(), False),\n",
    "  StructField('TITLE', StringType(), False),\n",
    "  StructField('VOTER_NAME', StringType(), False)\n",
    "])\n",
    "voter_df = spark.read.format('csv').options(Header=True).option(\"dateFormat\", \"mm/dd/yyyy\")\\\n",
    "    .load('DallasCouncilVoters.csv.gz', schema=voter_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b070a3-1ce3-4a8e-95ce-48889ff266fe",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26013c-4126-4594-982e-8067f463c47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T23:40:08.799098Z",
     "iopub.status.busy": "2023-04-01T23:40:08.798968Z",
     "iopub.status.idle": "2023-04-01T23:40:08.803331Z",
     "shell.execute_reply": "2023-04-01T23:40:08.802999Z",
     "shell.execute_reply.started": "2023-04-01T23:40:08.799087Z"
    }
   },
   "source": [
    "## What is caching?\n",
    "Caching in Spark:\n",
    "* Stores DataFrames in memory or on disk\n",
    "* Improves speed on later transformations / actions\n",
    "* Reduces resource usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51aed99-fafc-42e5-8286-d5d8cd60a667",
   "metadata": {},
   "source": [
    "## Disadvantages of caching\n",
    "* Very large data sets may not fit in memory\n",
    "* Local disk based caching may not be a performance improvement\n",
    "* Cached objects may not be available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d6c86-a8c7-49f2-a60d-d9913d46c9a3",
   "metadata": {},
   "source": [
    "## Caching tips\n",
    "* When developing Spark tasks:\n",
    "* Cache only if you need it\n",
    "* Try caching DataFrames at various points and determine if your performance improves\n",
    "* Cache in memory and fast SSD / NVMe storage\n",
    "* Cache to slow local disk if needed\n",
    "* Use intermediate files (e.g. parquet)!\n",
    "* Stop caching objects when finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fae655-3de4-4d73-a6a2-b86932f3080b",
   "metadata": {},
   "source": [
    "## Implementing caching\n",
    "Call .cache() on the DataFrame before Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d84199c-9ce1-4515-873d-afa968f0759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:40.647192Z",
     "iopub.status.busy": "2023-05-02T22:08:40.647035Z",
     "iopub.status.idle": "2023-05-02T22:08:40.649333Z",
     "shell.execute_reply": "2023-05-02T22:08:40.648945Z",
     "shell.execute_reply.started": "2023-05-02T22:08:40.647181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d53dc1-13da-4742-b4ba-3af15ee63d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:40.650249Z",
     "iopub.status.busy": "2023-05-02T22:08:40.650148Z",
     "iopub.status.idle": "2023-05-02T22:08:41.976541Z",
     "shell.execute_reply": "2023-05-02T22:08:41.976206Z",
     "shell.execute_reply.started": "2023-05-02T22:08:40.650239Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f4e498-3df8-43c7-b4d9-833513b8664d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:41.977505Z",
     "iopub.status.busy": "2023-05-02T22:08:41.977167Z",
     "iopub.status.idle": "2023-05-02T22:08:42.251758Z",
     "shell.execute_reply": "2023-05-02T22:08:42.251363Z",
     "shell.execute_reply.started": "2023-05-02T22:08:41.977483Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---+\n",
      "|      DATE|        TITLE|         VOTER_NAME| ID|\n",
      "+----------+-------------+-------------------+---+\n",
      "|2017-01-08|Councilmember|  Jennifer S. Gates|  0|\n",
      "|2017-01-08|Councilmember| Philip T. Kingston|  1|\n",
      "|2017-01-08|        Mayor|Michael S. Rawlings|  2|\n",
      "|2017-01-08|Councilmember|       Adam Medrano|  3|\n",
      "|2017-01-08|Councilmember|       Casey Thomas|  4|\n",
      "|2017-01-08|Councilmember|Carolyn King Arnold|  5|\n",
      "|2017-01-08|Councilmember|       Scott Griggs|  6|\n",
      "|2017-01-08|Councilmember|   B. Adam  McGough|  7|\n",
      "|2017-01-08|Councilmember|       Lee Kleinman|  8|\n",
      "|2017-01-08|Councilmember|      Sandy Greyson|  9|\n",
      "|2017-01-08|Councilmember|  Jennifer S. Gates| 10|\n",
      "|2017-01-08|Councilmember| Philip T. Kingston| 11|\n",
      "|2017-01-08|        Mayor|Michael S. Rawlings| 12|\n",
      "|2017-01-08|Councilmember|       Adam Medrano| 13|\n",
      "|2017-01-08|Councilmember|       Casey Thomas| 14|\n",
      "|2017-01-08|Councilmember|Carolyn King Arnold| 15|\n",
      "|2017-01-08|Councilmember| Rickey D. Callahan| 16|\n",
      "|2017-01-11|Councilmember|  Jennifer S. Gates| 17|\n",
      "|2018-01-25|Councilmember|     Sandy  Greyson| 18|\n",
      "|2018-01-25|Councilmember| Jennifer S.  Gates| 19|\n",
      "+----------+-------------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('ID', monotonically_increasing_id())\n",
    "voter_df = voter_df.cache()\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288b985-6616-478b-a739-0e2685acb4d2",
   "metadata": {},
   "source": [
    "## More cache operations\n",
    "Check .is_cached to determine cache status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78346d93-1b1c-40bd-91cd-d7a3eb86472a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:42.252305Z",
     "iopub.status.busy": "2023-05-02T22:08:42.252193Z",
     "iopub.status.idle": "2023-05-02T22:08:42.254924Z",
     "shell.execute_reply": "2023-05-02T22:08:42.254473Z",
     "shell.execute_reply.started": "2023-05-02T22:08:42.252294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(voter_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464284c-5d17-46f5-8355-5b9d3c5dd20e",
   "metadata": {},
   "source": [
    "Call .unpersist() when finished with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158e0e5f-75fa-4741-83a6-4a1e7da5b013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:42.255561Z",
     "iopub.status.busy": "2023-05-02T22:08:42.255347Z",
     "iopub.status.idle": "2023-05-02T22:08:42.267458Z",
     "shell.execute_reply": "2023-05-02T22:08:42.267046Z",
     "shell.execute_reply.started": "2023-05-02T22:08:42.255550Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "voter_df.unpersist()\n",
    "print(voter_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27dd1f-32ad-4854-9378-d6ad9636354d",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961714a1-1c1d-4abe-8078-7d4ab0b8a0e0",
   "metadata": {},
   "source": [
    "### Caching a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21af6085-bb22-4c23-a081-86a965777c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:42.268028Z",
     "iopub.status.busy": "2023-05-02T22:08:42.267919Z",
     "iopub.status.idle": "2023-05-02T22:08:42.473940Z",
     "shell.execute_reply": "2023-05-02T22:08:42.473531Z",
     "shell.execute_reply.started": "2023-05-02T22:08:42.268017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "departures_df = spark.read.format('csv').options(Header=True).load('AA_DFW_????_Departures_Short.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40834f0b-508b-4021-a0d9-bcc55d1cf6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:42.474646Z",
     "iopub.status.busy": "2023-05-02T22:08:42.474377Z",
     "iopub.status.idle": "2023-05-02T22:08:43.974192Z",
     "shell.execute_reply": "2023-05-02T22:08:43.973546Z",
     "shell.execute_reply.started": "2023-05-02T22:08:42.474633Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 583718 rows took 1.282351 seconds\n",
      "Counting 583718 rows again took 0.213948 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575477e-a4c4-403e-a801-77836745d3ee",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab4890f-cb4b-446e-89da-7ddcce36853a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:43.975132Z",
     "iopub.status.busy": "2023-05-02T22:08:43.974923Z",
     "iopub.status.idle": "2023-05-02T22:08:43.983101Z",
     "shell.execute_reply": "2023-05-02T22:08:43.982401Z",
     "shell.execute_reply.started": "2023-05-02T22:08:43.975114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ec21d-9dff-427a-bd00-cfcc6f477e8c",
   "metadata": {},
   "source": [
    "# Improve import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1b5e5-f691-46d2-8aa8-f1de73d8ff60",
   "metadata": {},
   "source": [
    "## Spark clusters\n",
    "Spark Clusters are made of two types of processes\n",
    "* Driver process\n",
    "* Worker processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de6d6a-97f8-491a-8626-15200e785d7d",
   "metadata": {},
   "source": [
    "## Import performance\n",
    "Important parameters:\n",
    "* Number of objects (Files, Network locations, etc)\n",
    "    * More objects better han larger ones\n",
    "    * Can import via wildcard  \n",
    "        `airport_df = spark.read.csv('airports-*.txt.gz')`\n",
    "    * General size of objects\n",
    "        * Spark performs better if objects are of similar size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef8a69-313f-4c34-868e-95d03bf5044b",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "A well-defined schema will drastically improve import performance\n",
    "* Avoids reading the data multiple times\n",
    "* Provides validation on import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d45c9b-2975-49c7-84f0-b2b2b590f6b1",
   "metadata": {},
   "source": [
    "## How to split objects\n",
    "* Use OS utilities / scripts (split, cut, awk)  \n",
    "        `split -l 10000 -d largefile chunk-`\n",
    "* Use custom scripts\n",
    "* Write out to Parquet\n",
    "\n",
    "        df_csv = spark.read.csv('singlelargefile.csv')\n",
    "        df_csv.write.parquet('data.parquet')\n",
    "        df = spark.read.parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b8e63-3e97-4831-9ea4-e3d2c54985fd",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39824bc2-93ca-479b-a3d2-3fd075de3648",
   "metadata": {},
   "source": [
    "### Saving multiples csv in a single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a343fe84-cbd8-43ae-b377-65218eacbe5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:43.984049Z",
     "iopub.status.busy": "2023-05-02T22:08:43.983701Z",
     "iopub.status.idle": "2023-05-02T22:08:44.209048Z",
     "shell.execute_reply": "2023-05-02T22:08:44.208379Z",
     "shell.execute_reply.started": "2023-05-02T22:08:43.984030Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583718"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departures_df = spark.read.format('csv').options(Header=True).load('AA_DFW_????_Departures_Short.csv.gz')\n",
    "departures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb45831-c9d3-42da-9769-81fbdb04fa03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:44.210271Z",
     "iopub.status.busy": "2023-05-02T22:08:44.209808Z",
     "iopub.status.idle": "2023-05-02T22:08:45.433425Z",
     "shell.execute_reply": "2023-05-02T22:08:45.433052Z",
     "shell.execute_reply.started": "2023-05-02T22:08:44.210244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "departures_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").option(\"compression\", \"gzip\").mode(\"overwrite\").save(\"departures_full.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a52d0a-8cc6-45cd-9355-65dfdc663388",
   "metadata": {},
   "source": [
    "### Spliting dataframe in multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a619893d-fe63-44d3-acf1-b80851cee543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:45.435765Z",
     "iopub.status.busy": "2023-05-02T22:08:45.435470Z",
     "iopub.status.idle": "2023-05-02T22:08:45.810036Z",
     "shell.execute_reply": "2023-05-02T22:08:45.809748Z",
     "shell.execute_reply.started": "2023-05-02T22:08:45.435744Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "departures_df.write.format(\"csv\").option(\"header\", \"true\").option(\"compression\", \"gzip\").mode(\"overwrite\").partitionBy().save(\"departures_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04434ed7-430b-420d-934e-44f716ce8680",
   "metadata": {},
   "source": [
    "### File import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18952ce4-9126-4dee-9bd6-1932338b1475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:45.810740Z",
     "iopub.status.busy": "2023-05-02T22:08:45.810491Z",
     "iopub.status.idle": "2023-05-02T22:08:46.149301Z",
     "shell.execute_reply": "2023-05-02T22:08:46.148951Z",
     "shell.execute_reply.started": "2023-05-02T22:08:45.810724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t583718\n",
      "Time to run: 0.142767\n",
      "Total rows in split DataFrame:\t583718\n",
      "Time to run: 0.080472\n"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz', header=True)\n",
    "split_df = spark.read.csv('departures_split/*.csv.gz', header=True)\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe9021-dd3e-42a1-9450-dff0160117a4",
   "metadata": {},
   "source": [
    "# Cluster configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd663ace-8b48-485c-8ca1-11375357f8d9",
   "metadata": {},
   "source": [
    "## Configuration options\n",
    "* Spark contains many configuration settings\n",
    "* These can be modified to macht needs\n",
    "* Reading configurion settings:\n",
    "\n",
    "        spark.conf.get(<configuration name>)\n",
    "\n",
    "* Writing configuration settins:\n",
    "\n",
    "        spark.conf.set(<configuration name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70abd41-8d04-4420-88ef-c327215817ba",
   "metadata": {},
   "source": [
    "## Cluster Types\n",
    "Spark deployment options:\n",
    "* Single node\n",
    "* Standalone\n",
    "* Managed\n",
    "    * YARN\n",
    "    * Mesos\n",
    "    * Kubernetes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a98455-c6f5-4370-872b-164a5e6464be",
   "metadata": {},
   "source": [
    "## Driver\n",
    "* Task assignment\n",
    "* Result consolidation\n",
    "* Shared data access\n",
    "\n",
    "Tips:\n",
    "* Driver node should have double the memory of the worker\n",
    "* Fast local storage helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39f50f-7355-4ee4-b983-dd1be88369a5",
   "metadata": {},
   "source": [
    "## Worker\n",
    "* Runs actual tasks\n",
    "* Ideally has all code, data, and resources for a given task\n",
    "\n",
    "Recommendations:\n",
    "* Depending on the task, more worker nodes is ofter better than larger workers\n",
    "* Test to find the balance\n",
    "* Fast local storage extremely useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5d792-e1f1-4b51-81fb-e806fdc88fe6",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a174f-544f-4b98-a581-9b19bef6f0f7",
   "metadata": {},
   "source": [
    "### Reading Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e95c5f-b68e-41fd-b43f-5f4756545cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:46.150101Z",
     "iopub.status.busy": "2023-05-02T22:08:46.149898Z",
     "iopub.status.idle": "2023-05-02T22:08:46.156001Z",
     "shell.execute_reply": "2023-05-02T22:08:46.155504Z",
     "shell.execute_reply.started": "2023-05-02T22:08:46.150082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 46629\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afda57d-290d-4aea-a6a3-9b839864ad5a",
   "metadata": {},
   "source": [
    "### Writing Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e399a5-725f-4b47-9f3b-1c63abe1cbb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:46.156826Z",
     "iopub.status.busy": "2023-05-02T22:08:46.156665Z",
     "iopub.status.idle": "2023-05-02T22:08:46.580468Z",
     "shell.execute_reply": "2023-05-02T22:08:46.580161Z",
     "shell.execute_reply.started": "2023-05-02T22:08:46.156810Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 4\n",
      "Partition count after change: 11\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures_split/', header=True).distinct()\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f64ea8cd-6222-46d2-95a7-d51ccf69af35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:46.581060Z",
     "iopub.status.busy": "2023-05-02T22:08:46.580840Z",
     "iopub.status.idle": "2023-05-02T22:08:46.997581Z",
     "shell.execute_reply": "2023-05-02T22:08:46.997097Z",
     "shell.execute_reply.started": "2023-05-02T22:08:46.581049Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583718"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departures_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a8a3e-fafa-4556-b47c-9407aa53e8a1",
   "metadata": {},
   "source": [
    "# Performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b899cd-44ab-4a80-ab9f-e44550c04dc3",
   "metadata": {},
   "source": [
    "## Explaining the Spark execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c39761a-8a14-4bd5-ac97-d0b56a71b91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:46.998666Z",
     "iopub.status.busy": "2023-05-02T22:08:46.998228Z",
     "iopub.status.idle": "2023-05-02T22:08:47.009050Z",
     "shell.execute_reply": "2023-05-02T22:08:47.008663Z",
     "shell.execute_reply.started": "2023-05-02T22:08:46.998644Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "voter_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('DATE', DateType(), False),\n",
    "  StructField('TITLE', StringType(), False),\n",
    "  StructField('VOTER_NAME', StringType(), False)\n",
    "])\n",
    "voter_df = spark.read.format('csv').options(Header=True).option(\"dateFormat\", \"mm/dd/yyyy\")\\\n",
    "    .load('DallasCouncilVoters.csv.gz', schema=voter_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8105f226-c056-4248-a7a4-7c3e70d4280c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.009569Z",
     "iopub.status.busy": "2023-05-02T22:08:47.009463Z",
     "iopub.status.idle": "2023-05-02T22:08:47.022229Z",
     "shell.execute_reply": "2023-05-02T22:08:47.021961Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.009558Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[VOTER_NAME#725], functions=[])\n",
      "   +- Exchange hashpartitioning(VOTER_NAME#725, 500), ENSURE_REQUIREMENTS, [plan_id=493]\n",
      "      +- HashAggregate(keys=[VOTER_NAME#725], functions=[])\n",
      "         +- InMemoryTableScan [VOTER_NAME#725]\n",
      "               +- InMemoryRelation [DATE#723, TITLE#724, VOTER_NAME#725], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- FileScan csv [DATE#0,TITLE#1,VOTER_NAME#2] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mauricio/code/big-data-with-pyspark/3-cleaning-data-with-py..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DATE:date,TITLE:string,VOTER_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.select(voter_df['VOTER_NAME']).distinct().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cee30-e074-446a-80d5-40981299d3d0",
   "metadata": {},
   "source": [
    "## What is shuffling?\n",
    "Shuffling referes to moving data around to various workers to complete a task.\n",
    "\n",
    "Spark distributes data amongst the various nodes in the cluster. A side effect of this is what is known as shuffling. Shuffling is the moving of data fragments to various workers as required to complete certain tasks.\n",
    "\n",
    "* Hides complexity from the user  \n",
    "    The user doesn't have to know which nodes have what data)(the user doesn't have to know which nodes have what data\n",
    "* Can be slow to complete the necessary transfers\n",
    "    Especially if a few nodes require all the data\n",
    "* Lowers overall throughput  \n",
    "    As the workers must spend time waiting for the data to transfer. This limits the amount of available workers for the remaining tasks in the system.\n",
    "* Is often necessary, but try to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745d619-fd2c-4d9e-9cd4-db51aac9ebd5",
   "metadata": {},
   "source": [
    "## How to limit shuffling?\n",
    "* Limit use of .repartition(num_partitions)\n",
    "    * Use `.coalesce(num_partitions)` instead\n",
    "* Use care when calling `.join()`  \n",
    "    Calling .join() indiscriminately can often cause shuffle operations, leading to increased cluster load & slower processing times. To avoid some of the shuffle operations when joining Spark DataFrames you can use the .broadcast().\n",
    "* Use `.broadcast()`\n",
    "* May not need to limit it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd29ff7-3344-44d1-906c-dd327ddcfd34",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "Broadcasting:\n",
    "* Provides a copy of an object to each worker\n",
    "* Prevents undue / excess communication between nodes\n",
    "* Can drastically speed up .join() operations  \n",
    "Use the `.broadcast(<DataFrame>)` method\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import broadcast\n",
    "combined_df = df_1.join(broadcast(df_2))\n",
    "```\n",
    "> Note broadcasting can slow operations when using very small DataFrames or if you broadcast the larger DataFrame in a join. Spark will often optimize this for you, but as usual, run tests in your environment for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d3b20-3c9a-424f-8764-321c424127c5",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde5f92-177b-423c-a859-7aa80fde28d2",
   "metadata": {},
   "source": [
    "### Normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768cc9a0-36ad-4e7e-bef1-7e46d3977496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.022700Z",
     "iopub.status.busy": "2023-05-02T22:08:47.022597Z",
     "iopub.status.idle": "2023-05-02T22:08:47.107947Z",
     "shell.execute_reply": "2023-05-02T22:08:47.107680Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.022689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2014|         0005|                HNL|                          519|\n",
      "|       01/01/2014|         0007|                OGG|                          505|\n",
      "|       01/01/2014|         0035|                SLC|                          174|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df = spark.read.format('csv').options(Header=True).load('AA_DFW_????_Departures_Short.csv.gz')\n",
    "flights_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e467fe6-b76c-4515-8aab-40b25f4322c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.108484Z",
     "iopub.status.busy": "2023-05-02T22:08:47.108311Z",
     "iopub.status.idle": "2023-05-02T22:08:47.208133Z",
     "shell.execute_reply": "2023-05-02T22:08:47.207780Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.108473Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df = spark.read.format('csv').options(Header=True).load('airports.csv')\n",
    "airports_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a05fab15-12e9-44a0-a6cb-270c6ac03b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.208702Z",
     "iopub.status.busy": "2023-05-02T22:08:47.208566Z",
     "iopub.status.idle": "2023-05-02T22:08:47.241218Z",
     "shell.execute_reply": "2023-05-02T22:08:47.240931Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.208692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#795], [faa#840], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#795)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#793,Flight Number#794,Destination Airport#795,Actual elapsed time (Minutes)#796] Batched: false, DataFilters: [isnotnull(Destination Airport#795)], Format: CSV, Location: InMemoryFileIndex(4 paths)[file:/home/mauricio/code/big-data-with-pyspark/3-cleaning-data-with-py..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=566]\n",
      "      +- Filter isnotnull(faa#840)\n",
      "         +- FileScan csv [faa#840,name#841,lat#842,lon#843,alt#844,tz#845,dst#846] Batched: false, DataFilters: [isnotnull(faa#840)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mauricio/code/big-data-with-pyspark/3-cleaning-data-with-py..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"faa\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff7694-52fd-46aa-9466-7954992df72e",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfd6a9-4bb0-4194-8572-145b33adea42",
   "metadata": {},
   "source": [
    "A couple tips:\n",
    "\n",
    "* Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "* On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "* If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb2e9b8-e8a0-4985-9fd1-8f7f73b0b7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.241799Z",
     "iopub.status.busy": "2023-05-02T22:08:47.241603Z",
     "iopub.status.idle": "2023-05-02T22:08:47.256912Z",
     "shell.execute_reply": "2023-05-02T22:08:47.256609Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.241787Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#795], [faa#840], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Destination Airport#795)\n",
      "   :  +- FileScan csv [Date (MM/DD/YYYY)#793,Flight Number#794,Destination Airport#795,Actual elapsed time (Minutes)#796] Batched: false, DataFilters: [isnotnull(Destination Airport#795)], Format: CSV, Location: InMemoryFileIndex(4 paths)[file:/home/mauricio/code/big-data-with-pyspark/3-cleaning-data-with-py..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=589]\n",
      "      +- Filter isnotnull(faa#840)\n",
      "         +- FileScan csv [faa#840,name#841,lat#842,lon#843,alt#844,tz#845,dst#846] Batched: false, DataFilters: [isnotnull(faa#840)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/mauricio/code/big-data-with-pyspark/3-cleaning-data-with-py..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"faa\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be479793-24c3-43af-8038-ade2b7682aec",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b4235e2-8ccc-4f52-a102-d3136b35e850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T22:08:47.257398Z",
     "iopub.status.busy": "2023-05-02T22:08:47.257300Z",
     "iopub.status.idle": "2023-05-02T22:08:47.736834Z",
     "shell.execute_reply": "2023-05-02T22:08:47.736520Z",
     "shell.execute_reply.started": "2023-05-02T22:08:47.257388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t581170\tduration: 0.274576\n",
      "Broadcast count:\t581170\tduration: 0.202525\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
