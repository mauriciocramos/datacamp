{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b993bb-a6fc-460a-af2a-fc6284746cde",
   "metadata": {},
   "source": [
    "# Why updating the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41decebf-f4dc-41a4-ba5a-af516105bead",
   "metadata": {},
   "source": [
    "* Better results on your specific domain\n",
    "* Learn classification schemes specifically for  your problem\n",
    "* Essential for text classification\n",
    "* Very useful for named entity recognition\n",
    "* Less critical for part-of-speech tagging and dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4adedb-6ef1-4773-9ddf-8aa036c83b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T14:58:45.691263Z",
     "iopub.status.busy": "2022-12-29T14:58:45.691134Z",
     "iopub.status.idle": "2022-12-29T14:58:45.695917Z",
     "shell.execute_reply": "2022-12-29T14:58:45.695538Z",
     "shell.execute_reply.started": "2022-12-29T14:58:45.691252Z"
    }
   },
   "source": [
    "# How training works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1c41e-83a2-49e7-aae1-b88fdd8c4f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T14:58:45.691263Z",
     "iopub.status.busy": "2022-12-29T14:58:45.691134Z",
     "iopub.status.idle": "2022-12-29T14:58:45.695917Z",
     "shell.execute_reply": "2022-12-29T14:58:45.695538Z",
     "shell.execute_reply.started": "2022-12-29T14:58:45.691252Z"
    }
   },
   "source": [
    "1. **Initialize** the model weights randomly with `nlp.begin_training`\n",
    "1. **Predict** a few examples with the current weights by calling `nlp.update`\n",
    "1. **Compare** prediction with true labels\n",
    "1. **Calculate** how to change weights to improve predictions\n",
    "1. **Update** weights slightly\n",
    "1. Go back to *2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1ae43-6c1d-437c-be1c-2cb12ef306c9",
   "metadata": {},
   "source": [
    "* *Training data:* Examples and their annotations.\n",
    "* *Text:* The input text the model should predict a label for.\n",
    "* *Label:* the label the model should predict.\n",
    "* *Gradient:* How to change the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2dcd30-abe5-4397-840f-dc5b36013e04",
   "metadata": {},
   "source": [
    "# Example: Training the entity recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad004d9-82e9-46b3-9c8c-13952f2a5c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T15:02:01.728427Z",
     "iopub.status.busy": "2022-12-29T15:02:01.728293Z",
     "iopub.status.idle": "2022-12-29T15:02:01.731054Z",
     "shell.execute_reply": "2022-12-29T15:02:01.730611Z",
     "shell.execute_reply.started": "2022-12-29T15:02:01.728416Z"
    },
    "tags": []
   },
   "source": [
    "* The entity recognizer tags words and phrases in context\n",
    "* Each token can only be part of one entity\n",
    "* Examples need to come with context\n",
    "```(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]}```\n",
    "* Texts with no entities are also important\n",
    "```(\"I need a new phone! Any tips?\", {'entities': []})```\n",
    "* **Goal:** teacht the model to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12d2a1-3d76-499f-9b43-61d58faf5253",
   "metadata": {},
   "source": [
    "# The training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427da58-c3aa-4d3f-a6cb-239d025694f4",
   "metadata": {},
   "source": [
    "* Example of what we ant the model to predict in context\n",
    "* Update an **exisiting model:** a few hundres to a few thousand of examples\n",
    "* Traing a **new category:** a few thousand to a million examples\n",
    "    * spaCy's English models: 2 million words\n",
    "* Usually created manually by human annotators\n",
    "* Can bem semi-automated - for , using spaCy's `matcher`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc08357-b3b2-47e8-9e31-664c98a11f6d",
   "metadata": {},
   "source": [
    "# Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a89fc92-b006-4338-95f4-cfe0fb9f9a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:50.429421Z",
     "iopub.status.busy": "2022-12-29T17:25:50.429333Z",
     "iopub.status.idle": "2022-12-29T17:25:53.747595Z",
     "shell.execute_reply": "2022-12-29T17:25:53.747256Z",
     "shell.execute_reply.started": "2022-12-29T17:25:50.429410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af06fc94-c9b7-4d8d-bf1a-eabda532bd75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:53.748508Z",
     "iopub.status.busy": "2022-12-29T17:25:53.748363Z",
     "iopub.status.idle": "2022-12-29T17:25:53.750925Z",
     "shell.execute_reply": "2022-12-29T17:25:53.750676Z",
     "shell.execute_reply.started": "2022-12-29T17:25:53.748497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET_RULES', [pattern1, pattern2], greedy='FIRST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d2fc-a9a2-45ae-952b-42908f0cfb67",
   "metadata": {},
   "source": [
    "Let's use the match patterns we've created in the previous exercise to bootstrap a set of training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a1f13e-9dda-4c9b-89e9-47c163749246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:53.751394Z",
     "iopub.status.busy": "2022-12-29T17:25:53.751289Z",
     "iopub.status.idle": "2022-12-29T17:25:53.753308Z",
     "shell.execute_reply": "2022-12-29T17:25:53.753080Z",
     "shell.execute_reply.started": "2022-12-29T17:25:53.751384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"How to preorder the iPhone X\",\n",
    "    \"iPhone X is coming\",\n",
    "    \"Should I pay $1,000 for the iPhone X?\",\n",
    "    \"The iPhone 8 reviews are here\",\n",
    "    \"Your iPhone goes up to 11 today\",\n",
    "    \"I need a new phone! Any tips?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcf20da-2e21-4ed5-87d7-8f2d987de7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:53.753696Z",
     "iopub.status.busy": "2022-12-29T17:25:53.753605Z",
     "iopub.status.idle": "2022-12-29T17:25:54.262579Z",
     "shell.execute_reply": "2022-12-29T17:25:54.262161Z",
     "shell.execute_reply.started": "2022-12-29T17:25:53.753687Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to preorder the iPhone X [(4, 6, 'GADGET')]\n",
      "iPhone X is coming [(0, 2, 'GADGET')]\n",
      "Should I pay $1,000 for the iPhone X? [(7, 9, 'GADGET')]\n",
      "The iPhone 8 reviews are here [(1, 3, 'GADGET')]\n",
      "Your iPhone goes up to 11 today [(1, 2, 'GADGET')]\n",
      "I need a new phone! Any tips? []\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Find the matches in the doc\n",
    "    matches = matcher(doc)\n",
    "    # Get a list of (start, end, label) tuples of matches in the text\n",
    "    entities = [(start, end, 'GADGET') for match_id, start, end in matches]\n",
    "    print(doc.text, entities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf19d7c6-6019-4a11-ba1c-dc81825f4561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:54.263075Z",
     "iopub.status.busy": "2022-12-29T17:25:54.262970Z",
     "iopub.status.idle": "2022-12-29T17:25:54.277203Z",
     "shell.execute_reply": "2022-12-29T17:25:54.276893Z",
     "shell.execute_reply.started": "2022-12-29T17:25:54.263065Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA:\n",
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print('TRAINING DATA:')\n",
    "print(*TRAINING_DATA, sep='\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d18c7-1ff6-4d2c-a3e3-ef65ccad3bc0",
   "metadata": {},
   "source": [
    "Before you train a model with the data, you always want to double-check that your matcher didn't identify any false positives. But that process is still much faster than doing everything manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302e628-bb64-4150-bbed-5ace254b7d87",
   "metadata": {},
   "source": [
    "# The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558cb3c-e00f-48fd-b044-f613e14670c8",
   "metadata": {},
   "source": [
    "## The steps of a training loop\n",
    "1. **Loop** for a number of times.\n",
    "    1. **Shuffle** the training data.\n",
    "        * This is a very common strategy when doing stochastic gradient descent\n",
    "    1. **Divide** the data into batches (**minibatching**)\n",
    "        * This makes it esasier to make a more accurate estimate of the gradient\n",
    "    1. **Update** the model for each batch.\n",
    "1. **Save** the updated model.\n",
    "    * to a directory and use it in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e953-57aa-44c8-826c-9a18b0733c13",
   "metadata": {},
   "source": [
    "## Recap: How training works\n",
    "\n",
    "* **Training data:** Examples and their annotations.\n",
    "* **Text:** The input text the model should predict a label for.\n",
    "* **Label:** The label the model should predict.\n",
    "* **Gradient:** How to change the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e76b3d-ca25-49c5-88d4-07fb3e8f1694",
   "metadata": {},
   "source": [
    "## Example loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc7afe0-dc9b-4bb7-84f4-28a077a971b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:54.277741Z",
     "iopub.status.busy": "2022-12-29T17:25:54.277605Z",
     "iopub.status.idle": "2022-12-29T17:25:55.726320Z",
     "shell.execute_reply": "2022-12-29T17:25:55.726005Z",
     "shell.execute_reply.started": "2022-12-29T17:25:54.277731Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.893784943974425}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.880575732397553}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.622654999275502}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.62265507602581}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 13.34633607948837}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.806273358892062}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.5741935175658563}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.57419405182385}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.812813435502919}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.8874090640282146}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.250660509876805}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.324660833347172}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.2402845573857508}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.397578547860606}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.59278660832926}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.645522977210448}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.645530911807418}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.719180007560132}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.096349401563927}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.3460200783928293}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.3880479925333917}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.3880601590013857}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.518816935982274}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.4591665000959955}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.0492040980091275e-05}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.5870962901559333}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.116525460820902}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.429484035391063}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 7.591633131508168}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.239176567022001}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.0691626243451635}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.0694522955640853}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 4.558752115135983}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.143669949505776}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 8.097017960596274}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 10.40479651974121}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.8359528019037725}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.1804535354921057}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.501555276798044}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.9069054511487025}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.9072277758783045}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 2.991581054151588}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.16469245469272664}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.25440415415152984}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.34852485779424924}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.3487586864623289}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.7724181388559508}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.7776544465623054}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 6.934424489202229e-05}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.00318095289601672}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.008257380754024268}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.5476876578552279}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.5503395986747535}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.5534560687238752}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0007875959671892474}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0014241959121674697}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0014475895205686151}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0016049199422920046}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.3401626206156263}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1.3403746303827515}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses)\n",
    "            print(losses)\n",
    "# Save the model\n",
    "# nlp.to_disk('example_model_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5c71d-a11e-4790-aaaf-2c856bd70cc7",
   "metadata": {},
   "source": [
    "# Updating an existing model\n",
    "* Improve the predictions on new data\n",
    "* Especially useful to improve existing categories, like `PERSON`\n",
    "* Also possible to add new categories\n",
    "* Be careful and make sure the model doesn't \"forget\" the old ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970d011-a8de-4c59-a1a2-61f47ea7a578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T16:23:31.430272Z",
     "iopub.status.busy": "2022-12-29T16:23:31.430095Z",
     "iopub.status.idle": "2022-12-29T16:23:31.432705Z",
     "shell.execute_reply": "2022-12-29T16:23:31.432382Z",
     "shell.execute_reply.started": "2022-12-29T16:23:31.430260Z"
    }
   },
   "source": [
    "## Setting up a new pipeline from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ad55a0-7f8a-411e-b945-be68f8f88ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:55.727288Z",
     "iopub.status.busy": "2022-12-29T17:25:55.727178Z",
     "iopub.status.idle": "2022-12-29T17:25:55.794067Z",
     "shell.execute_reply": "2022-12-29T17:25:55.793762Z",
     "shell.execute_reply.started": "2022-12-29T17:25:55.727277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': 'en', 'name': 'pipeline', 'version': '0.0.0', 'spacy_version': '>=3.4.3,<3.5.0', 'description': '', 'author': '', 'email': '', 'url': '', 'license': '', 'spacy_git_version': '63673a792', 'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None, 'mode': 'default'}, 'labels': {}, 'pipeline': [], 'components': [], 'disabled': []}\n"
     ]
    }
   ],
   "source": [
    "from spacy import Language\n",
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "print(nlp.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4528eb-8f9b-413d-a790-b6ccf9f3c734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:55.794558Z",
     "iopub.status.busy": "2022-12-29T17:25:55.794456Z",
     "iopub.status.idle": "2022-12-29T17:25:55.803138Z",
     "shell.execute_reply": "2022-12-29T17:25:55.802832Z",
     "shell.execute_reply.started": "2022-12-29T17:25:55.794548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.ner.EntityRecognizer at 0x7fb0291b6e40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create blank entity recognizer\n",
    "ner = nlp.create_pipe('ner')\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414d7eaf-c265-499a-a47e-aa3fce6554d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:55.803647Z",
     "iopub.status.busy": "2022-12-29T17:25:55.803514Z",
     "iopub.status.idle": "2022-12-29T17:25:55.811704Z",
     "shell.execute_reply": "2022-12-29T17:25:55.811412Z",
     "shell.execute_reply.started": "2022-12-29T17:25:55.803637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fb0291b6f20>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add component to the pipeline\n",
    "nlp.add_pipe('ner')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a3e1dc-fb20-4fa9-9d29-e05be43b8034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:55.812189Z",
     "iopub.status.busy": "2022-12-29T17:25:55.812082Z",
     "iopub.status.idle": "2022-12-29T17:25:55.814546Z",
     "shell.execute_reply": "2022-12-29T17:25:55.814210Z",
     "shell.execute_reply.started": "2022-12-29T17:25:55.812178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GADGET',)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new label to the ner component\n",
    "ner.add_label('GADGET')\n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd847a1d-dd01-4be4-87be-f35d6713286b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:55.815030Z",
     "iopub.status.busy": "2022-12-29T17:25:55.814925Z",
     "iopub.status.idle": "2022-12-29T17:25:56.318032Z",
     "shell.execute_reply": "2022-12-29T17:25:56.317747Z",
     "shell.execute_reply.started": "2022-12-29T17:25:55.815021Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 13.333333969116211}\n",
      "{'ner': 22.191051602363586}\n",
      "{'ner': 33.834924817085266}\n",
      "{'ner': 7.21665346622467}\n",
      "{'ner': 17.289491593837738}\n",
      "{'ner': 26.563148707151413}\n",
      "{'ner': 7.375452399253845}\n",
      "{'ner': 11.601762011647224}\n",
      "{'ner': 14.12882811576128}\n",
      "{'ner': 1.320203622803092}\n",
      "{'ner': 3.078902713721618}\n",
      "{'ner': 11.463523261016235}\n",
      "{'ner': 9.077382935211062}\n",
      "{'ner': 10.918071497697383}\n",
      "{'ner': 14.191691261250526}\n",
      "{'ner': 2.2280071023851633}\n",
      "{'ner': 3.266769803361967}\n",
      "{'ner': 5.278516272548586}\n",
      "{'ner': 0.741884347567975}\n",
      "{'ner': 2.0866386377383606}\n",
      "{'ner': 2.326729200380214}\n",
      "{'ner': 0.05624298445036402}\n",
      "{'ner': 0.09393305278581465}\n",
      "{'ner': 1.5297884328711007}\n",
      "{'ner': 0.0011251797302520572}\n",
      "{'ner': 1.525631058919771}\n",
      "{'ner': 1.526033094552261}\n",
      "{'ner': 0.0002986922327750108}\n",
      "{'ner': 1.097896041893799}\n",
      "{'ner': 1.0979152700196462}\n"
     ]
    }
   ],
   "source": [
    "# Start the training to initialize the model with random weights\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses={}\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        # Update the model\n",
    "        nlp.update([Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch],\n",
    "                   losses=losses)      \n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65aa7ee-9ea4-41f0-aa30-5ab5f879817c",
   "metadata": {},
   "source": [
    "> The numbers printed represent the loss on each iteration, the amount of work left for the optimizer.  \n",
    "The lower the number, the better.  \n",
    "In real life, you normally want to use a lot more data than this, ideally at least a few hundred or a few thousand examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c2396-f6e1-4d9f-b821-fcdb75a0556d",
   "metadata": {},
   "source": [
    "## Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7b0ba9-055e-4fab-9f69-012e1ce120a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:56.318552Z",
     "iopub.status.busy": "2022-12-29T17:25:56.318436Z",
     "iopub.status.idle": "2022-12-29T17:25:56.320583Z",
     "shell.execute_reply": "2022-12-29T17:25:56.320318Z",
     "shell.execute_reply.started": "2022-12-29T17:25:56.318537Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_DATA = [\n",
    "    \"Apple is slowing down the iPhone 8 and iPhone X - how to stop it\",\n",
    "    \"I finally understand what the iPhone X 'notch' is for\",\n",
    "    \"Everything you need to know about the Samsung Galaxy S9\",\n",
    "    \"Looking to compare iPad models? Here’s how the 2018 lineup stacks up\",\n",
    "    \"The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\",\n",
    "    \"what is the cheapest ipad, especially ipad pro???\",\n",
    "    \"Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e6f6b8-fa28-48e3-85d1-beaa3d53923c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T17:25:56.321065Z",
     "iopub.status.busy": "2022-12-29T17:25:56.320954Z",
     "iopub.status.idle": "2022-12-29T17:25:56.328700Z",
     "shell.execute_reply": "2022-12-29T17:25:56.328437Z",
     "shell.execute_reply.started": "2022-12-29T17:25:56.321054Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "[('iPhone 8', 'GADGET'), ('iPhone X', 'GADGET')] \n",
      "\n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "[('iPhone X', 'GADGET')] \n",
      "\n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "[] \n",
      "\n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "[] \n",
      "\n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "[('iPhone 8', 'GADGET'), ('iPhone 8', 'GADGET')] \n",
      "\n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "[] \n",
      "\n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "[] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print([(ent.text, ent.label_ )for ent in doc.ents], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f36e5c-baf0-4344-be79-7f81096a9a6d",
   "metadata": {},
   "source": [
    "# Training best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7129185-7cd6-421e-8ac5-26e4762a7a46",
   "metadata": {},
   "source": [
    "## Problem 1: Models can \"forget\" things\n",
    "\n",
    "Statistical models can learn lots of things – but it doesn't mean that they won't unlearn them. If you're updating an existing model with new data, especially new labels, it can overfit and adjust *too much* to the new examples. For instance, if you're only updating it with examples of \"website\", it may \"forget\" other labels it previously predicted correctly – like \"person\". This is also known as the catastrophic forgetting problem. \n",
    "\n",
    "* Existing model can overfit on new data\n",
    "* e.g.: if you only update it with WEBSITE , it can \"unlearn\" what a PERSON is\n",
    "* Also known as \"catastrophic forgetting\" problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbd381-e3af-4023-8a92-2f2ef1ea672d",
   "metadata": {},
   "source": [
    "## Solution 1: Mix in previously correct predictions\n",
    "\n",
    "To prevent this, make sure to always mix in examples of what the model previously got correct. If you're training a new category \"website\", also include examples of \"person\". spaCy can help you with this. You can create those additional examples by running the existing model over data and extracting the entity spans you care about. You can then mix those examples in with your existing data and update the model with annotations of all labels. \n",
    "\n",
    "* **For example**, if you're training `WEBSITE`, also include examples of `PERSON`  \n",
    "* Run existing spaCy model over data and extract all other relevant entities\n",
    "\n",
    "**BAD:**\n",
    "\n",
    "    TRAINING_DATA = [  \n",
    "        (Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})]\n",
    "\n",
    "**GOOD:**\n",
    "\n",
    "    TRAINING_DATA = [  \n",
    "        ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),  \n",
    "        ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36b7fa-046f-4056-a4fb-cadda3461839",
   "metadata": {},
   "source": [
    "## Problem 2: Models can't learn everything\n",
    "\n",
    "Another common problem is that your model just won't learn what you want it to. spaCy's models make predictions based on the **local context** – for example,\n",
    "\n",
    "for named entities, the surrounding words are most important. If the decision is difficult to make based on the context, the model can struggle to learn it.\n",
    "\n",
    "The label scheme also needs to be consistent and not too specific.\n",
    "\n",
    "For example, it may be very difficult to teach a model to predict whether something is adult clothing or children's clothing based on the context. However, just predicting the label \"clothing\" may work better. \n",
    "\n",
    "* spaCy's models make predictions based on **local context**\n",
    "* Model can struggle to learn if decision is difficult to make based on context\n",
    "* Label scheme needs to be consistent and not too specific\n",
    "    * For example: CLOTHING is better than ADULT_CLOTHING and CHILDRENS_CLOTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4b68c-55f7-4573-a296-7a3a747d949a",
   "metadata": {},
   "source": [
    "## Solution 2: Plan your label scheme carefully\n",
    "\n",
    "Before you start training and updating models, it's worth taking a step back and planning your label scheme.\n",
    "\n",
    "Try to pick **categories** that are reflected in the **local context** and make them more **generic** if possible.\n",
    "\n",
    "You can always add a **rule-based system** later to go from **generic** to **specific**.\n",
    "\n",
    "**Generic categories** like \"clothing\" or \"band\" are both easier to label and easier to learn.\n",
    "\n",
    "* Pick categories that are reflected in **local context**\n",
    "* More **generic is better than too specific**\n",
    "* Use rules to go from **generic labels** to **specific categories**\n",
    "\n",
    "**BAD:**\n",
    "\n",
    "    LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
    "    \n",
    "**GOOD:**\n",
    "\n",
    "    LABELS = ['CLOTHING', 'BAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1fa5-4be7-4ae3-8fb8-81496efcef80",
   "metadata": {},
   "source": [
    "## Good data vs. bad data\n",
    "\n",
    "Here's an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews.\n",
    "\n",
    "TOURIST_DESTINATION\n",
    "\n",
    "    ('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'TOURIST_DESTINATION')]})\n",
    "    ('You should visit Paris once in your life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'TOURIST_DESTINATION')]})\n",
    "    (\"There's also a Paris in Arkansas, lol\", {'entities': []})\n",
    "    ('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'TOURIST_DESTINATION')]})\n",
    "\n",
    "Why is this data and label scheme problematic?\n",
    "\n",
    "    Whether a place is a tourist destination is a subjective judgement and not a definitive category. It will be very difficult for the entity recognizer to learn.\n",
    "    \n",
    "Rewrite the TRAINING_DATA to only use the label GPE (cities, states, countries) instead of TOURIST_DESTINATION.  \n",
    "Don't forget to add tuples for the GPE entities that weren't labeled in the old data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12449c01-e9ad-4915-a9dc-720f5ba8be12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:51:08.879935Z",
     "iopub.status.busy": "2022-12-29T18:51:08.879597Z",
     "iopub.status.idle": "2022-12-29T18:51:08.882516Z",
     "shell.execute_reply": "2022-12-29T18:51:08.882216Z",
     "shell.execute_reply.started": "2022-12-29T18:51:08.879921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'GPE')]})\n",
      "('You should visit Paris once in your life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'GPE')]})\n",
      "(\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24, 32, 'GPE')]})\n",
      "('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'GPE')]})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"i went to amsterdem last year and the canals were beautiful\", {'entities': [(10, 19, 'GPE')]}),\n",
    "    (\"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\", {'entities': [(17, 22, 'GPE')]}),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24,32, 'GPE')]}),\n",
    "    (\"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\", {'entities': [(0, 6, 'GPE')]})\n",
    "]\n",
    "     \n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e66631-bc60-4ada-8b48-74429cf7330b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:51:38.535177Z",
     "iopub.status.busy": "2022-12-29T18:51:38.535033Z",
     "iopub.status.idle": "2022-12-29T18:51:38.538082Z",
     "shell.execute_reply": "2022-12-29T18:51:38.537747Z",
     "shell.execute_reply.started": "2022-12-29T18:51:38.535166Z"
    }
   },
   "source": [
    "> Once the model achieves good results on detecting GPE entities in the traveler reviews, you could add a rule-based component to determine whether the entity is a tourist destination in this context.  \n",
    "> For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61aa8cf-4670-45e2-ba2f-bb396c9ac940",
   "metadata": {},
   "source": [
    "## Training multiple labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22a2ce-3e21-4938-b98f-cfbd11127a7f",
   "metadata": {},
   "source": [
    "Here's a small sample of a dataset created to train a new entity type WEBSITE. The original dataset contains a few thousand sentences. In this exercise, you'll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, Brat (https://brat.nlplab.org/), a popular open-source solution, or Prodigy (https://prodi.gy/), our own annotation tool that integrates with spaCy.\n",
    "\n",
    "After this exercise you will be nearly done with the course! If you enjoyed it, feel free to send Ines a thank you via Twitter - she'll appreciate it! Tweet to Ines (http://twitter.com/home?status=Thoroughly%20enjoyed%20the%20Advanced%20NLP%20With%20spaCy%20course%20%40DataCamp%20by%20%40_inesmontani.%20https%3A%2F%2Fbit.ly/2DTUzxP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de5a09-1d0d-4058-8feb-ea923ab3a286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T20:22:29.270748Z",
     "iopub.status.busy": "2022-12-29T20:22:29.270426Z",
     "iopub.status.idle": "2022-12-29T20:22:29.273429Z",
     "shell.execute_reply": "2022-12-29T20:22:29.273084Z",
     "shell.execute_reply.started": "2022-12-29T20:22:29.270734Z"
    }
   },
   "source": [
    "Complete the entity offsets for the WEBSITE entities in the data. Feel free to use len() if you don't want to count the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fac86-1c42-4a6a-9664-2a4c43cf4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43114bf6-324b-416d-90fb-d356c55b3ac9",
   "metadata": {},
   "source": [
    "Update the training data to include annotations for the PERSON entities \"PewDiePie\" and \"Alexis Ohanian\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827354d-d0da-44a3-a88f-26deb6e2e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(0, 9, 'PERSON'), (18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (15, 29, 'PERSON')]}),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab9542-6bf2-48d2-b3d4-8b29313e2506",
   "metadata": {},
   "source": [
    "# Your new spaCy skills\n",
    "\n",
    "In the first chapter, you learned how to extract linguistic features like part-of-speech tags, syntactic dependencies and named entities, and how to work with pre-trained statistical models. You also learned to write powerful match patterns to extract words and phrases using spaCy's matcher and phrase matcher.\n",
    "\n",
    "Chapter 2 was all about information extraction, and you learned how to work with the data structures, the Doc, Token and Span, as well as the vocab and lexical entries. You also used spaCy to predict semantic similarities using word vectors.\n",
    "\n",
    "In chapter 3, you got some more insights into spaCy's pipeline, and learned to write your own custom pipeline components that modify the Doc. You also created your own custom extension attributes for Docs, Tokens and Spans, and learned about processing streams and making your pipeline faster.\n",
    "\n",
    "Finally, in chapter 4, you learned about training and updating spaCy's statistical models, specifically the entity recognizer. You learned some useful tricks for how to create training data, and how to design your label scheme to get the best results. \n",
    "\n",
    "* Chapter 1\n",
    "    * Extract linguistic features: part-of-speech tags, dependencies, named entities\n",
    "    * Work with pre-trained statistical models\n",
    "    * Find words and phrases using Matcher and PhraseMatcher match rules\n",
    "* Chapter 2\n",
    "    * Best practices for working with data structures Doc , Token Span , Vocab , Lexeme\n",
    "    * Find semantic similarities using word vectors\n",
    "* Chapter 3\n",
    "    * Write custom pipeline components with extension a attributes\n",
    "    * Scale up your spaCy pipelines and make them fast\n",
    "* Chapter 4\n",
    "    * Create training data for spaCy' statistical models\n",
    "    * Train and update spaCy's neural network models with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637ceeb-ad83-4225-a450-56f1ca749951",
   "metadata": {},
   "source": [
    "## More things to do with spaCy\n",
    "\n",
    "Of course, there's a lot more that spaCy can do that we didn't get to cover in this course. While we focused mostly on training the entity recognizer, you can also train and update the other statistical pipeline components like the part-of-speech tagger and dependency parser. Another useful pipeline component is the text classifier, which can learn to predict labels that apply to the whole text. It's not part of the pre-trained models, but you can add it to an existing model and train it on your own data. \n",
    "\n",
    "* Training (https://spacy.io/usage/training) and updating other pipeline components\n",
    "    * Part-of-speech tagger\n",
    "    * Dependency parser\n",
    "    * Text classifier\n",
    "    \n",
    "In this course, we basically accepted the default tokenization as it is. But you don't have to! spaCy lets you customize the rules used to determine where and how to split the text. You can also add and improve the support for other languages. While spaCy already supports tokenization for many different languages, there's still a lot of room for improvement. Supporting tokenization for a new language is the first step towards being able to train a statistical model. \n",
    "\n",
    "* Customizing the tokenizer (https://spacy.io/usage/linguistic-features#tokenization)\n",
    "    * Adding rules and exceptions to split text differently\n",
    "* Adding or improving support for other languages (https://spacy.io/usage/adding-languages)\n",
    "    * 45+ languages currently\n",
    "    * Lots of room for improvement and more languages\n",
    "    * Allows training models for other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633eef11-baf0-4366-94c3-0900452dece2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
